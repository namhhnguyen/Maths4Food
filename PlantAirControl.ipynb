{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Import"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","import copy\n","import cma\n","import gymnasium as gym\n","import numpy as np\n","\n","from gymnasium.spaces import Dict, Box, Discrete, MultiDiscrete\n","\n","from stable_baselines3 import PPO\n","from stable_baselines3.common.vec_env import DummyVecEnv\n","from stable_baselines3.common.env_util import make_vec_env\n","from stable_baselines3.common.callbacks import CheckpointCallback\n"]},{"cell_type":"markdown","metadata":{},"source":["### Utils"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def is_between(A, b):\n","    return A[0] < b <= A[1]\n","\n","def calculate_soil_evap_rate(T, R):\n","    A0 = 1.0\n","    A1 = 0.1\n","    return np.float32(A0*(1 + A1*T)*(1-R*0.01))\n","\n","def calculate_plant_evap_rate(T, R, light_on_off):\n","    A2 = 1.0\n","    A3 = 0.1\n","    A4 = 0.1\n","    return np.float32(A2*(1 + A3*T)*(1-R*0.01)*(1+A4*light_on_off))"]},{"cell_type":"markdown","metadata":{},"source":["### PlantAirControl Environment"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["class PlantAirControl(gym.Env):\n","    # Constants are hard-coded for now but can set up to read from a spreadsheet\n","    CHAMBER_VOLUME = 1.0\n","    NON_LIGHT_HEAT = 0.5\n","    LIGHT_HEAT = 1.0\n","\n","    FAN_MAX_WATTAGE = 0.5 # kW\n","    HEAT_MAX_WATTAGE = 3 # kW\n","    COOL_MAX_WATTAGE = 3 # kW\n","\n","    FAN_MAX_AIR_INOUT_RATE = 1.0 \n","    HEAT_MAX_WATER_TEMP_UP_RATE = 1.0\n","    COOL_MAX_WATER_TEMP_DOWN_RATE = 1.0\n","\n","    DESIRED_TEMPS = [25.0, 32.0] # Celcius\n","    DESIRED_HUMIDS = [70.0, 85.0] # Percentage humidity\n","\n","    def __init__(self, render_mode=None,\n","                light_heat = LIGHT_HEAT,\n","                non_light_heat = NON_LIGHT_HEAT,\n","                chamber_volume = CHAMBER_VOLUME,\n","\n","                fan_max_wattage = FAN_MAX_WATTAGE,\n","                heat_max_wattage = HEAT_MAX_WATTAGE,\n","                cool_max_wattage = COOL_MAX_WATTAGE,\n","\n","                fan_max_air_inout_rate = FAN_MAX_AIR_INOUT_RATE,\n","                heat_max_water_temp_up_rate = HEAT_MAX_WATER_TEMP_UP_RATE,\n","                cool_max_water_temp_down_rate = COOL_MAX_WATER_TEMP_DOWN_RATE,\n","\n","                desired_temps = DESIRED_TEMPS,\n","                desired_humids = DESIRED_HUMIDS,\n","                ):\n","        # Set up some variables\n","        self.chamber_volume = np.float32(chamber_volume)\n","        self.light_heat = np.float32(light_heat)\n","        self.non_light_heat = np.float32(non_light_heat)\n","\n","        self.fan_max_wattage = np.float32(fan_max_wattage)\n","        self.heat_max_wattage = np.float32(heat_max_wattage)\n","        self.cool_max_wattage = np.float32(cool_max_wattage)\n","\n","        self.fan_max_air_inout_rate = np.float32(fan_max_air_inout_rate)\n","        self.heat_max_water_temp_up_rate = np.float32(heat_max_water_temp_up_rate)\n","        self.cool_max_water_temp_down_rate = np.float32(cool_max_water_temp_down_rate)\n","\n","        self.dersired_temps = np.float32(np.array(desired_temps))\n","        self.dersired_humids = np.float32(np.array(desired_humids))\n","\n","        # Specify the action_space\n","        self.action_space = MultiDiscrete([101]*3) # e.g., fan capacity, heating component capacity, cooling component capacity\n","        \n","        # Speicfy the observation_space\n","        self.observation_space = Dict({\"InTemp/InHumid/OutTemp/OutHumid/Energy\": Box(-100, 100, shape=(5,)),\n","                                       \"LightOnOff\": Discrete(2),\n","                                        \"Status\": Discrete(2),})\n","        \n","        # # Logger objects for longer time scale\n","        # self.inside_temps = []\n","        # self.inside_humids = []\n","        # self.outside_temps = []\n","        # self.outside_humids = []\n","        # self.fan_controls = []\n","        # self.heat_controls = []\n","        # self.cool_controls = []\n","\n","    def reset(self, seed=None):\n","        super().reset(seed=seed) # To enable self.np_random seeding\n","        rng = self.np_random\n","\n","        # Start state initialisation\n","        self.light_on_off = rng.integers(2)\n","\n","        self.inside_temp = rng.uniform(low = 15, high = 45)\n","        self.inside_humid = rng.uniform(low = 40, high = 90)\n","        self.outside_temp = rng.uniform(low = 0, high = 30)\n","        self.outside_humid = rng.uniform(low = 30, high = 70)\n","\n","        self.energy = 0\n","        \n","        self.plant_OK = 0 # 0 means not in the box yet, 1 means OK\n","        \n","        observation = {\"InTemp/InHumid/OutTemp/OutHumid/Energy\": np.float32(np.array([self.inside_temp, \n","                                                                          self.inside_humid, \n","                                                                          self.outside_temp,\n","                                                                          self.outside_humid,\n","                                                                          self.energy])),\n","                        \"LightOnOff\": self.light_on_off,\n","                        \"Status\": self.plant_OK}\n","\n","        info = {}\n","\n","        return observation, info\n","\n","    def step(self, action):\n","        # # Log things for longer time scale\n","        # self.inside_temps.append(self.inside_temp)\n","        # self.inside_humids.append(self.inside_humid)\n","        # self.outside_temps.append(self.outside_temp)\n","        # self.outside_humids.append(self.outside_humid)\n","        # self.fan_controls.append(action[0])\n","        # self.heat_controls.append(action[1])\n","        # self.cool_controls.append(action[2])\n","\n","        # Caculate some quantities\n","        self.plant_evap_rate = calculate_plant_evap_rate(self.inside_temp, self.inside_humid, self.light_on_off)\n","        self.soil_evap_rate = calculate_soil_evap_rate(self.inside_temp, self.inside_humid)\n","\n","        # Interprete action into parameters of mathematical model\n","        self.fan_air_inout_rate = (action[0] * self.fan_max_air_inout_rate)/100\n","        self.heat_water_temp_up_rate = (action[1] * self.heat_max_water_temp_up_rate)/100\n","        self.cool_water_temp_down_rate = (action[2] * self.cool_max_water_temp_down_rate)/100\n","\n","        self.fan_wattage = (action[0] * self.fan_max_wattage)/100\n","        self.heat_wattage = (action[1] * self.heat_max_wattage)/100\n","        self.cool_wattage = (action[2] * self.cool_max_wattage)/100\n","\n","        # Update observations via model for temp, humididty\n","        self.inside_temp += (self.non_light_heat + self.light_on_off * self.light_heat)/self.chamber_volume \\\n","            + 5.0 * self.fan_air_inout_rate * (self.outside_temp - self.inside_temp) \\\n","            + 5.0 * (2*self.heat_water_temp_up_rate - self.cool_water_temp_down_rate)\n","\n","        self.inside_humid += (self.plant_evap_rate + self.soil_evap_rate)/self.chamber_volume \\\n","            + 5.0 * self.fan_air_inout_rate * (self.outside_humid - self.inside_humid) \\\n","            + 5.0 * (self.heat_water_temp_up_rate - 2*self.cool_water_temp_down_rate)\n","\n","        self.energy += self.fan_wattage + self.heat_wattage + self.cool_wattage\n","\n","        if is_between(self.dersired_temps, self.inside_temp) and is_between(self.dersired_humids, self.inside_humid):\n","            self.plant_OK = 1\n","\n","        observation = {\"InTemp/InHumid/OutTemp/OutHumid/Energy\": np.float32(np.array([self.inside_temp, \n","                                                                          self.inside_humid, \n","                                                                          self.outside_temp,\n","                                                                          self.outside_humid,\n","                                                                          self.energy])),\n","                        \"LightOnOff\": self.light_on_off,\n","                        \"Status\": self.plant_OK}\n","\n","        # Reward\n","        reward = self.plant_OK * 10 - self.energy\n","\n","        terminated = True\n","\n","        info = {}\n","            \n","        return observation, reward, terminated, False, info"]},{"cell_type":"markdown","metadata":{},"source":["### Testing"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([22.600061, 67.65732 , 26.657713, 64.71329 ,  0.      ],\n","      dtype=float32), 'LightOnOff': 1, 'Status': 0}\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([37.099064, 72.06908 , 26.657713, 64.71329 ,  2.995   ],\n","      dtype=float32), 'LightOnOff': 1, 'Status': 0}\n","The final reward is -2.9949999999999997\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([28.222027, 76.547356, 29.562115, 68.03025 ,  0.      ],\n","      dtype=float32), 'LightOnOff': 0, 'Status': 0}\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([32.099163, 65.788666, 29.562115, 68.03025 ,  0.545   ],\n","      dtype=float32), 'LightOnOff': 0, 'Status': 0}\n","The final reward is -0.545\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([36.408325, 47.318813,  7.525201, 46.402637,  0.      ],\n","      dtype=float32), 'LightOnOff': 0, 'Status': 0}\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([-21.13455 ,  52.492916,   7.525201,  46.402637,   2.14    ],\n","      dtype=float32), 'LightOnOff': 0, 'Status': 0}\n","The final reward is -2.14\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([18.930168, 66.617874,  9.412023, 53.159298,  0.      ],\n","      dtype=float32), 'LightOnOff': 1, 'Status': 0}\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([ 0.09206112, 39.232937  ,  9.412023  , 53.159298  ,  0.69      ],\n","      dtype=float32), 'LightOnOff': 1, 'Status': 0}\n","The final reward is -0.69\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([44.133568, 78.64099 , 24.36311 , 51.534058,  0.      ],\n","      dtype=float32), 'LightOnOff': 1, 'Status': 0}\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([-32.59088 , -34.08537 ,  24.36311 ,  51.534058,   3.875   ],\n","      dtype=float32), 'LightOnOff': 1, 'Status': 0}\n","The final reward is -3.875\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([32.99011  , 64.0572   ,  3.7371545, 67.227936 ,  0.       ],\n","      dtype=float32), 'LightOnOff': 1, 'Status': 0}\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([-43.205517 ,  76.72161  ,   3.7371545,  67.227936 ,   1.505    ],\n","      dtype=float32), 'LightOnOff': 1, 'Status': 0}\n","The final reward is -1.505\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([30.87986 , 46.26309 , 20.774168, 64.607346,  0.      ],\n","      dtype=float32), 'LightOnOff': 1, 'Status': 0}\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([ -9.33803 , 143.76315 ,  20.774168,  64.607346,   3.37    ],\n","      dtype=float32), 'LightOnOff': 1, 'Status': 0}\n","The final reward is -3.3699999999999997\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([22.31178  , 44.052433 ,  1.7785237, 66.75309  ,  0.       ],\n","      dtype=float32), 'LightOnOff': 0, 'Status': 0}\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([12.565164 , 56.623394 ,  1.7785237, 66.75309  ,  5.405    ],\n","      dtype=float32), 'LightOnOff': 0, 'Status': 0}\n","The final reward is -5.404999999999999\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([18.845346, 78.896484, 24.745722, 41.944294,  0.      ],\n","      dtype=float32), 'LightOnOff': 0, 'Status': 0}\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([ 38.246475, -30.742607,  24.745722,  41.944294,   1.02    ],\n","      dtype=float32), 'LightOnOff': 0, 'Status': 0}\n","The final reward is -1.02\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([33.312992, 45.61096 , 25.941803, 59.481285,  0.      ],\n","      dtype=float32), 'LightOnOff': 1, 'Status': 0}\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([11.460781, 80.50792 , 25.941803, 59.481285,  2.7     ],\n","      dtype=float32), 'LightOnOff': 1, 'Status': 0}\n","The final reward is -2.7\n"]}],"source":["env = PlantAirControl()\n","episodes = 10\n","for episode in range(1, episodes + 1):\n","    obsINI, infoINI  = env.reset()\n","    print(obsINI)\n","    score = 0\n","    terminated = False\n","    truncated = False\n","\n","    while not terminated or truncated:\n","        action = env.action_space.sample()\n","        obs, reward, terminated, truncated, info = env.step(action)\n","        print(obs)\n","        print(f\"The final reward is {reward}\")"]},{"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["log_path = os.path.join(os.getcwd(), \"Logs\")\n","save_path = os.path.join(os.getcwd(), \"Saved Models\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"57daaa42406941929ebea0171d99e647","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 24\u001b[0m\n\u001b[1;32m     16\u001b[0m checkpoint_callback \u001b[38;5;241m=\u001b[39m CheckpointCallback(save_freq\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e6\u001b[39m, \n\u001b[1;32m     17\u001b[0m                                            save_path\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCheckpoints\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     18\u001b[0m                                            name_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCheckpoint\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     19\u001b[0m                                            save_replay_buffer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m                                            save_vecnormalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m                                            verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1e7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[1;32m     27\u001b[0m model_final_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal\u001b[39m\u001b[38;5;124m'\u001b[39m) \n","File \u001b[0;32m~/anaconda3/envs/RayEnv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/RayEnv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:281\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mrecord(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime/total_timesteps\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps, exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdump(step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps)\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n","File \u001b[0;32m~/anaconda3/envs/RayEnv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:210\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_sde:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mreset_noise(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m--> 210\u001b[0m values, log_prob, entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrollout_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Normalize advantage\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/RayEnv/lib/python3.10/site-packages/stable_baselines3/common/policies.py:699\u001b[0m, in \u001b[0;36mActorCriticPolicy.evaluate_actions\u001b[0;34m(self, obs, actions)\u001b[0m\n\u001b[1;32m    697\u001b[0m     latent_pi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor\u001b[38;5;241m.\u001b[39mforward_actor(pi_features)\n\u001b[1;32m    698\u001b[0m     latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor\u001b[38;5;241m.\u001b[39mforward_critic(vf_features)\n\u001b[0;32m--> 699\u001b[0m distribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_action_dist_from_latent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_pi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mlog_prob(actions)\n\u001b[1;32m    701\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net(latent_vf)\n","File \u001b[0;32m~/anaconda3/envs/RayEnv/lib/python3.10/site-packages/stable_baselines3/common/policies.py:662\u001b[0m, in \u001b[0;36mActorCriticPolicy._get_action_dist_from_latent\u001b[0;34m(self, latent_pi)\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist\u001b[38;5;241m.\u001b[39mproba_distribution(action_logits\u001b[38;5;241m=\u001b[39mmean_actions)\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist, MultiCategoricalDistribution):\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;66;03m# Here mean_actions are the flattened logits\u001b[39;00m\n\u001b[0;32m--> 662\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproba_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist, BernoulliDistribution):\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;66;03m# Here mean_actions are the logits (before rounding to get the binary actions)\u001b[39;00m\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist\u001b[38;5;241m.\u001b[39mproba_distribution(action_logits\u001b[38;5;241m=\u001b[39mmean_actions)\n","File \u001b[0;32m~/anaconda3/envs/RayEnv/lib/python3.10/site-packages/stable_baselines3/common/distributions.py:342\u001b[0m, in \u001b[0;36mMultiCategoricalDistribution.proba_distribution\u001b[0;34m(self, action_logits)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mproba_distribution\u001b[39m(\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfMultiCategoricalDistribution, action_logits: th\u001b[38;5;241m.\u001b[39mTensor\n\u001b[1;32m    341\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfMultiCategoricalDistribution:\n\u001b[0;32m--> 342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribution \u001b[38;5;241m=\u001b[39m [Categorical(logits\u001b[38;5;241m=\u001b[39msplit) \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m th\u001b[38;5;241m.\u001b[39msplit(action_logits, \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dims), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n","File \u001b[0;32m~/anaconda3/envs/RayEnv/lib/python3.10/site-packages/stable_baselines3/common/distributions.py:342\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mproba_distribution\u001b[39m(\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfMultiCategoricalDistribution, action_logits: th\u001b[38;5;241m.\u001b[39mTensor\n\u001b[1;32m    341\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfMultiCategoricalDistribution:\n\u001b[0;32m--> 342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribution \u001b[38;5;241m=\u001b[39m [\u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m th\u001b[38;5;241m.\u001b[39msplit(action_logits, \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dims), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n","File \u001b[0;32m~/anaconda3/envs/RayEnv/lib/python3.10/site-packages/torch/distributions/categorical.py:64\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`logits` parameter must be at least one-dimensional.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# Normalize\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;241m-\u001b[39m \u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogsumexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs \u001b[38;5;28;01mif\u001b[39;00m probs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":[" # Set up the training environment\n","n_envs = 8\n","envs = make_vec_env(PlantAirControl, \n","                    n_envs=n_envs,\n","                    seed=42,\n","                    vec_env_cls=DummyVecEnv)\n","\n","# Set up the training model\n","model = PPO(\"MultiInputPolicy\", \n","            envs, \n","            verbose=0, \n","            tensorboard_log= os.path.join(log_path, \"Training\"), \n","            device = \"cpu\")\n","\n","# Set up checkpoints for during training\n","checkpoint_callback = CheckpointCallback(save_freq= 1e5, \n","                                            save_path=os.path.join(save_path, 'Checkpoints'),\n","                                            name_prefix='Checkpoint',\n","                                            save_replay_buffer=True,\n","                                            save_vecnormalize=True,\n","                                            verbose = 0)\n","\n","# Training\n","model.learn(2e6, callback = checkpoint_callback, progress_bar = True)\n","\n","# Save the model\n","model_final_path = os.path.join(save_path, 'Final') \n","model.save(model_final_path)"]},{"cell_type":"markdown","metadata":{},"source":["### Finetune with Evolutionary Strategies"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_final_path = os.path.join(os.getcwd(), \"Saved Models\", 'Final')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Turn RL problem into classic optimisation problem\n","class evaluate_action:\n","    def __init__(self, env, obsINI):\n","        self.env = env\n","        self.obsINI = obsINI\n","\n","    def __call__(self, action):\n","        action = np.floor(action)\n","        env_copy = copy.deepcopy(self.env)\n","        # Reset environment\n","        score = 0\n","        terminated = False\n","        truncated = False\n","        while not terminated or truncated:\n","            obs, reward, terminated, truncated, info = env_copy.step(action)\n","            score += reward\n","        fitness = 10 - score # This turns a maximisation problem into a minimsation problem\n","        return fitness"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fine-tune model with Evolutionary Strategies\n","def ESOptimisation(env, obsINI):\n","    # Copy action\n","    model = PPO.load(model_final_path, env)\n","    action, _  = model.predict(obsINI, deterministic = True)\n","\n","    # Orginal model score\n","    env_copy = copy.deepcopy(env)\n","    original_score = 0\n","    terminated = False\n","    truncated = False\n","    while not terminated or truncated:\n","        obs, reward, terminated, truncated, info = env_copy.step(action)\n","        original_score += reward\n","\n","    # Finetune\n","    env_copy = copy.deepcopy(env)\n","    fun = evaluate_action(env_copy, obsINI)\n","    upper_buffer = np.float32(np.array([1 - 1e-7] * 3)) # Add buffer to give equal prob for Max action in ES \n","    upper_bounds = np.float32(np.array([100]*3)) + upper_buffer\n","    lower_bounds = np.float32(np.array([0] * 3))\n","\n","    print(f\"Fine-tuning in progress ...\")\n","    # ES starting from no action\n","    x, es= cma.fmin2(fun, action, 25,\n","                      {'integer_variables': list(range(len(lower_bounds))),\n","                       'bounds': [lower_bounds, upper_bounds], \n","                       'tolflatfitness':10, \n","                       'tolfun': 1e-6,\n","                       'tolfunhist': 1e-7,\n","                       'verbose': -10},\n","                        restarts=2,\n","                        eval_initial_x = True,\n","                        )\n","\n","    final_score = 10 - fun(x)\n","    final_action = np.floor(x)\n","\n","    return final_action"]},{"cell_type":"markdown","metadata":{},"source":["### Deployment"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create an environment\n","env = PlantAirControl()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load RL model\n","model_final_path = os.path.join(os.getcwd(), \"Saved Models\", 'Final')\n","model = PPO.load(model_final_path, env)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-0.7549999999999999\n","-0.87\n","-0.905\n","-0.64\n","-0.375\n"]}],"source":["episodes = 5\n","for episode in range(1, episodes + 1):\n","    obsINI, infoINI  = env.reset()\n","    score = 0\n","    terminated = False\n","    truncated = False\n","\n","    while not terminated or truncated:\n","        action, _  = model.predict(obsINI, deterministic = False)\n","        obs, reward, terminated, truncated, info = env.step(action)\n","        score += reward\n","        print(f\"The final score is {score}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fine-tuning in progress ...\n","The final score is 8.505\n","8.505\n","Fine-tuning in progress ...\n","The final score is 0.0\n","0.0\n","Fine-tuning in progress ...\n","The final score is 0.0\n","0.0\n","Fine-tuning in progress ...\n","The final score is 0.0\n","0.0\n","Fine-tuning in progress ...\n","The final score is 0.0\n","0.0\n"]}],"source":["episodes = 5\n","for episode in range(1, episodes + 1):\n","    obsINI, infoINI  = env.reset()\n","    action = ESOptimisation(env, obsINI)\n","    score = 0\n","    terminated = False\n","    truncated = False\n","\n","    while not terminated or truncated:\n","        obs, reward, terminated, truncated, info = env.step(action)\n","        score += reward\n","        print(f\"The final score is {score}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"RayEnv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":2}
