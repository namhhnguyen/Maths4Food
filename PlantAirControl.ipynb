{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Import"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","import copy\n","import cma\n","import gymnasium as gym\n","import numpy as np\n","\n","from gymnasium.spaces import Dict, Box, Discrete, MultiDiscrete\n","\n","from stable_baselines3 import PPO\n","from stable_baselines3.common.vec_env import DummyVecEnv\n","from stable_baselines3.common.env_util import make_vec_env\n","from stable_baselines3.common.callbacks import CheckpointCallback\n"]},{"cell_type":"markdown","metadata":{},"source":["### Utils"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def is_between(A, b):\n","    return A[0] < b <= A[1]\n","\n","def calculate_soil_evap_rate(T, R):\n","    A0 = 1.0\n","    A1 = 0.1\n","    return np.float32(A0*(1 + A1*T)*(1-R*0.01))\n","\n","def calculate_plant_evap_rate(T, R, light_on_off):\n","    A2 = 1.0\n","    A3 = 0.1\n","    A4 = 0.1\n","    return np.float32(A2*(1 + A3*T)*(1-R*0.01)*(1+A4*light_on_off))"]},{"cell_type":"markdown","metadata":{},"source":["### PlantAirControl Environment"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["class PlantAirControl(gym.Env):\n","    # Constants are hard-coded for now but can set up to read from a spreadsheet\n","    CHAMBER_VOLUME = 1.0\n","    NON_LIGHT_HEAT = 0.5\n","    LIGHT_HEAT = 1.0\n","\n","    FAN_MAX_WATTAGE = 0.5 # kW\n","    HEAT_MAX_WATTAGE = 3 # kW\n","    COOL_MAX_WATTAGE = 3 # kW\n","\n","    FAN_MAX_AIR_INOUT_RATE = 1.0 \n","    HEAT_MAX_WATER_TEMP_UP_RATE = 1.0\n","    COOL_MAX_WATER_TEMP_DOWN_RATE = 1.0\n","\n","    DESIRED_TEMPS = [25.0, 32.0] # Celcius\n","    DESIRED_HUMIDS = [70.0, 85.0] # Percentage humidity\n","\n","    def __init__(self, render_mode=None,\n","                light_heat = LIGHT_HEAT,\n","                non_light_heat = NON_LIGHT_HEAT,\n","                chamber_volume = CHAMBER_VOLUME,\n","\n","                fan_max_wattage = FAN_MAX_WATTAGE,\n","                heat_max_wattage = HEAT_MAX_WATTAGE,\n","                cool_max_wattage = COOL_MAX_WATTAGE,\n","\n","                fan_max_air_inout_rate = FAN_MAX_AIR_INOUT_RATE,\n","                heat_max_water_temp_up_rate = HEAT_MAX_WATER_TEMP_UP_RATE,\n","                cool_max_water_temp_down_rate = COOL_MAX_WATER_TEMP_DOWN_RATE,\n","\n","                desired_temps = DESIRED_TEMPS,\n","                desired_humids = DESIRED_HUMIDS,\n","                ):\n","        # Set up some variables\n","        self.chamber_volume = np.float32(chamber_volume)\n","        self.light_heat = np.float32(light_heat)\n","        self.non_light_heat = np.float32(non_light_heat)\n","\n","        self.fan_max_wattage = np.float32(fan_max_wattage)\n","        self.heat_max_wattage = np.float32(heat_max_wattage)\n","        self.cool_max_wattage = np.float32(cool_max_wattage)\n","\n","        self.fan_max_air_inout_rate = np.float32(fan_max_air_inout_rate)\n","        self.heat_max_water_temp_up_rate = np.float32(heat_max_water_temp_up_rate)\n","        self.cool_max_water_temp_down_rate = np.float32(cool_max_water_temp_down_rate)\n","\n","        self.dersired_temps = np.float32(np.array(desired_temps))\n","        self.dersired_humids = np.float32(np.array(desired_humids))\n","\n","        # Specify the action_space\n","        self.action_space = MultiDiscrete([101]*3) # e.g., fan capacity, heating component capacity, cooling component capacity\n","        \n","        # Speicfy the observation_space\n","        self.observation_space = Dict({\"InTemp/InHumid/OutTemp/OutHumid/Energy\": Box(-100, 100, shape=(5,)),\n","                                       \"LightOnOff\": Discrete(2),\n","                                        \"Status\": Discrete(2),})\n","        \n","        # # Logger objects for longer time scale\n","        # self.inside_temps = []\n","        # self.inside_humids = []\n","        # self.outside_temps = []\n","        # self.outside_humids = []\n","        # self.fan_controls = []\n","        # self.heat_controls = []\n","        # self.cool_controls = []\n","\n","    def reset(self, seed=None):\n","        super().reset(seed=seed) # To enable self.np_random seeding\n","        rng = self.np_random\n","\n","        # Start state initialisation\n","        self.light_on_off = rng.integers(2)\n","\n","        self.inside_temp = rng.uniform(low = 15, high = 45)\n","        self.inside_humid = rng.uniform(low = 40, high = 90)\n","        self.outside_temp = rng.uniform(low = 0, high = 30)\n","        self.outside_humid = rng.uniform(low = 30, high = 70)\n","\n","        self.energy = 0\n","        \n","        self.plant_OK = 0 # 0 means not in the box yet, 1 means OK\n","        \n","        observation = {\"InTemp/InHumid/OutTemp/OutHumid/Energy\": np.float32(np.array([self.inside_temp, \n","                                                                          self.inside_humid, \n","                                                                          self.outside_temp,\n","                                                                          self.outside_humid,\n","                                                                          self.energy])),\n","                        \"LightOnOff\": self.light_on_off,\n","                        \"Status\": self.plant_OK}\n","\n","        info = {}\n","\n","        return observation, info\n","\n","    def step(self, action):\n","        # # Log things for longer time scale\n","        # self.inside_temps.append(self.inside_temp)\n","        # self.inside_humids.append(self.inside_humid)\n","        # self.outside_temps.append(self.outside_temp)\n","        # self.outside_humids.append(self.outside_humid)\n","        # self.fan_controls.append(action[0])\n","        # self.heat_controls.append(action[1])\n","        # self.cool_controls.append(action[2])\n","\n","        # Caculate some quantities\n","        self.plant_evap_rate = calculate_plant_evap_rate(self.inside_temp, self.inside_humid, self.light_on_off)\n","        self.soil_evap_rate = calculate_soil_evap_rate(self.inside_temp, self.inside_humid)\n","\n","        # Interprete action into parameters of mathematical model\n","        self.fan_air_inout_rate = (action[0] * self.fan_max_air_inout_rate)/100\n","        self.heat_water_temp_up_rate = (action[1] * self.heat_max_water_temp_up_rate)/100\n","        self.cool_water_temp_down_rate = (action[2] * self.cool_max_water_temp_down_rate)/100\n","\n","        self.fan_wattage = (action[0] * self.fan_max_wattage)/100\n","        self.heat_wattage = (action[1] * self.heat_max_wattage)/100\n","        self.cool_wattage = (action[2] * self.cool_max_wattage)/100\n","\n","        # Update observations via model for temp, humididty\n","        self.inside_temp += (self.non_light_heat + self.light_on_off * self.light_heat)/self.chamber_volume \\\n","            + 5.0 * self.fan_air_inout_rate * (self.outside_temp - self.inside_temp) \\\n","            + 5.0 * (2*self.heat_water_temp_up_rate - self.cool_water_temp_down_rate)\n","\n","        self.inside_humid += (self.plant_evap_rate + self.soil_evap_rate)/self.chamber_volume \\\n","            + 5.0 * self.fan_air_inout_rate * (self.outside_humid - self.inside_humid) \\\n","            + 5.0 * (self.heat_water_temp_up_rate - 2*self.cool_water_temp_down_rate)\n","\n","        self.energy += self.fan_wattage + self.heat_wattage + self.cool_wattage\n","\n","        if is_between(self.dersired_temps, self.inside_temp) and is_between(self.dersired_humids, self.inside_humid):\n","            self.plant_OK = 1\n","\n","        observation = {\"InTemp/InHumid/OutTemp/OutHumid/Energy\": np.float32(np.array([self.inside_temp, \n","                                                                          self.inside_humid, \n","                                                                          self.outside_temp,\n","                                                                          self.outside_humid,\n","                                                                          self.energy])),\n","                        \"LightOnOff\": self.light_on_off,\n","                        \"Status\": self.plant_OK}\n","\n","        # Reward\n","        reward = self.plant_OK * 10 - self.energy\n","\n","        terminated = True\n","\n","        info = {}\n","            \n","        return observation, reward, terminated, False, info"]},{"cell_type":"markdown","metadata":{},"source":["### Testing"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([33.484715, 52.59845 ,  8.851879, 49.81481 ,  0.      ],\n","      dtype=float32), 'LightOnOff': 0, 'Status': 0}\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([-73.218124,  43.76211 ,   8.851879,  49.81481 ,   0.915   ],\n","      dtype=float32), 'LightOnOff': 0, 'Status': 0}\n","The final reward is -0.915\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([43.39585 , 61.122036, 24.763231, 61.021526,  0.      ],\n","      dtype=float32), 'LightOnOff': 0, 'Status': 0}\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([-42.140728,  63.791428,  24.763231,  61.021526,   3.12    ],\n","      dtype=float32), 'LightOnOff': 0, 'Status': 0}\n","The final reward is -3.12\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([26.011854, 72.22217 , 18.324625, 58.112957,  0.      ],\n","      dtype=float32), 'LightOnOff': 0, 'Status': 0}\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([-0.28706336, 13.535984  , 18.324625  , 58.112957  ,  5.32      ],\n","      dtype=float32), 'LightOnOff': 0, 'Status': 0}\n","The final reward is -5.32\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([33.722855, 78.511215, 18.727262, 48.94619 ,  0.      ],\n","      dtype=float32), 'LightOnOff': 0, 'Status': 0}\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([28.674397, 62.39256 , 18.727262, 48.94619 ,  4.445   ],\n","      dtype=float32), 'LightOnOff': 0, 'Status': 0}\n","The final reward is -4.445\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([27.223646, 74.985016, 16.217009, 49.069344,  0.      ],\n","      dtype=float32), 'LightOnOff': 0, 'Status': 0}\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([-18.756887, -44.26479 ,  16.217009,  49.069344,   4.09    ],\n","      dtype=float32), 'LightOnOff': 0, 'Status': 0}\n","The final reward is -4.09\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([22.877031, 43.128338, 24.619473, 69.01772 ,  0.      ],\n","      dtype=float32), 'LightOnOff': 0, 'Status': 0}\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([ 26.021769, 102.55794 ,  24.619473,  69.01772 ,   2.455   ],\n","      dtype=float32), 'LightOnOff': 0, 'Status': 0}\n","The final reward is -2.455\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([18.210543, 67.93015 , 18.951752, 47.49156 ,  0.      ],\n","      dtype=float32), 'LightOnOff': 0, 'Status': 0}\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([25.167713, -4.523579, 18.951752, 47.49156 ,  4.935   ],\n","      dtype=float32), 'LightOnOff': 0, 'Status': 0}\n","The final reward is -4.9350000000000005\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([34.057926, 60.306587, 28.353647, 51.99874 ,  0.      ],\n","      dtype=float32), 'LightOnOff': 0, 'Status': 0}\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([ 9.812596, 24.391926, 28.353647, 51.99874 ,  1.855   ],\n","      dtype=float32), 'LightOnOff': 0, 'Status': 0}\n","The final reward is -1.855\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([35.88931 , 63.61532 ,  2.522733, 60.685825,  0.      ],\n","      dtype=float32), 'LightOnOff': 1, 'Status': 0}\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([-108.52861 ,   56.142433,    2.522733,   60.685825,    2.585   ],\n","      dtype=float32), 'LightOnOff': 1, 'Status': 0}\n","The final reward is -2.585\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([23.932705, 53.487312, 29.34023 , 41.26556 ,  0.      ],\n","      dtype=float32), 'LightOnOff': 1, 'Status': 0}\n","{'InTemp/InHumid/OutTemp/OutHumid/Energy': array([24.732706, 53.001747, 29.34023 , 41.26556 ,  1.86    ],\n","      dtype=float32), 'LightOnOff': 1, 'Status': 0}\n","The final reward is -1.8599999999999999\n"]}],"source":["env = PlantAirControl()\n","episodes = 10\n","for episode in range(1, episodes + 1):\n","    obsINI, infoINI  = env.reset()\n","    print(obsINI)\n","    score = 0\n","    terminated = False\n","    truncated = False\n","\n","    while not terminated or truncated:\n","        action = env.action_space.sample()\n","        obs, reward, terminated, truncated, info = env.step(action)\n","        print(obs)\n","        print(f\"The final reward is {reward}\")"]},{"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["log_path = os.path.join(os.getcwd(), \"Logs\")\n","save_path = os.path.join(os.getcwd(), \"Saved Models\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3b15bda54b414310bd4878f1324432f9","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"],"text/plain":[]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n","</pre>\n"],"text/plain":["\n"]},"metadata":{},"output_type":"display_data"}],"source":[" # Set up the training environment\n","n_envs = 8\n","envs = make_vec_env(PlantAirControl, \n","                    n_envs=n_envs,\n","                    seed=42,\n","                    vec_env_cls=DummyVecEnv)\n","\n","# Set up the training model\n","model = PPO(\"MultiInputPolicy\", \n","            envs, \n","            verbose=0, \n","            tensorboard_log= os.path.join(log_path, \"Training\"), \n","            device = \"cpu\")\n","\n","# Set up checkpoints for during training\n","checkpoint_callback = CheckpointCallback(save_freq= 1e5, \n","                                            save_path=os.path.join(save_path, 'Checkpoints'),\n","                                            name_prefix='Checkpoint',\n","                                            save_replay_buffer=True,\n","                                            save_vecnormalize=True,\n","                                            verbose = 0)\n","\n","# Training\n","model.learn(2e6, callback = checkpoint_callback, progress_bar = True)\n","\n","# Save the model\n","model_final_path = os.path.join(save_path, 'Final') \n","model.save(model_final_path)"]},{"cell_type":"markdown","metadata":{},"source":["### Finetune with Evolutionary Strategies"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["model_final_path = os.path.join(os.getcwd(), \"Saved Models\", 'Final')"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Turn RL problem into classic optimisation problem\n","class evaluate_action:\n","    def __init__(self, env, obsINI):\n","        self.env = env\n","        self.obsINI = obsINI\n","\n","    def __call__(self, action):\n","        action = np.floor(action)\n","        env_copy = copy.deepcopy(self.env)\n","        # Reset environment\n","        score = 0\n","        terminated = False\n","        truncated = False\n","        while not terminated or truncated:\n","            obs, reward, terminated, truncated, info = env_copy.step(action)\n","            score += reward\n","        fitness = 10 - score # This turns a maximisation problem into a minimsation problem\n","        return fitness"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["# Fine-tune model with Evolutionary Strategies\n","def ESOptimisation(env, obsINI):\n","    # Copy action\n","    model = PPO.load(model_final_path, env)\n","    action, _  = model.predict(obsINI, deterministic = True)\n","\n","    # Orginal model score\n","    env_copy = copy.deepcopy(env)\n","    original_score = 0\n","    terminated = False\n","    truncated = False\n","    while not terminated or truncated:\n","        obs, reward, terminated, truncated, info = env_copy.step(action)\n","        original_score += reward\n","\n","    # Finetune\n","    env_copy = copy.deepcopy(env)\n","    fun = evaluate_action(env_copy, obsINI)\n","    upper_buffer = np.float32(np.array([1 - 1e-7] * 3)) # Add buffer to give equal prob for Max action in ES \n","    upper_bounds = np.float32(np.array([100]*3)) + upper_buffer\n","    lower_bounds = np.float32(np.array([0] * 3))\n","\n","    # ES starting from no action\n","    x, es= cma.fmin2(fun, action, 25,\n","                      {'integer_variables': list(range(len(lower_bounds))),\n","                       'bounds': [lower_bounds, upper_bounds], \n","                       'tolflatfitness':10, \n","                       'tolfun': 1e-6,\n","                       'tolfunhist': 1e-7,\n","                       'verbose': -10},\n","                        restarts=2,\n","                        eval_initial_x = True,\n","                        )\n","\n","    final_score = 10 - fun(x)\n","    final_action = np.floor(x)\n","\n","    return final_action"]},{"cell_type":"markdown","metadata":{},"source":["### Deployment"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["# Create an environment\n","env = PlantAirControl()"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["# Load RL model\n","model_final_path = os.path.join(os.getcwd(), \"Saved Models\", 'Final')\n","model = PPO.load(model_final_path, env)"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The fine-tuned score is 0.0\n","The fine-tuned score is 10.0\n","The fine-tuned score is 9.58\n","The fine-tuned score is 9.985\n","The fine-tuned score is 0.0\n","The fine-tuned score is 0.0\n","The fine-tuned score is 0.0\n","The fine-tuned score is 9.63\n","The fine-tuned score is 9.73\n","The fine-tuned score is 9.31\n"]}],"source":["episodes = 10\n","for episode in range(1, episodes + 1):\n","    obsINI, infoINI  = env.reset(seed = episode)\n","    action = ESOptimisation(env, obsINI)\n","    score = 0\n","    terminated = False\n","    truncated = False\n","\n","    while not terminated or truncated:\n","        obs, reward, terminated, truncated, info = env.step(action)\n","        score += reward\n","        print(f\"The fine-tuned score is {score}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"RayEnv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":2}
